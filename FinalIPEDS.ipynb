{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9kPzNZX3jO5QydJyzoCyT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkuntz/Complete-Python-3-Bootcamp/blob/master/FinalIPEDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZn612BNfKkY"
      },
      "outputs": [],
      "source": [
        "import requests \n",
        "from zipfile import ZipFile\n",
        "from google.colab import files\n",
        "import os\n",
        "import os.path\n",
        "from glob import glob\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import csv\n",
        "import glob    \n",
        "from google.colab import files\n",
        "import time\n",
        "import math\n",
        "# shutil.rmtree(\"temporary_csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install XlsxWriter "
      ],
      "metadata": {
        "id": "Sid10kLefMam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade xlrd"
      ],
      "metadata": {
        "id": "7UoaJFaDfMeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IGNORE THIS CELL UNLESS YOU NEED TO RUN IT TO START OVER AGAIN BY REMOVING ALL FILES. \n",
        "#  !rm -rf /content/IPEDS_FolderDict\n",
        "#  !rm -rf /content/IPEDS_Folder\n",
        "# !rm -rf //content/BDict2013.zip\n",
        "# !rm -rf //content/BDict2021.zip\n",
        "# !rm -rf //content/BDict2020.zip\n",
        "# !rm -rf //content/BDict2019.zip\n",
        "# !rm -rf //content/BDict2018.zip\n",
        "# !rm -rf //content/BDict2017.zip\n",
        "# !rm -rf //content/BDict2016.zip\n",
        "# !rm -rf //content/BDict2015.zip\n",
        "# !rm -rf //content/BDict2014.zip\n",
        "# !rm -rf //content/BDict2013.zip\n",
        "# !rm -rf //content/BDict2012.zip\n",
        "# !rm -rf //content/BDict2011.zip\n",
        "# !rm -rf //content/BDict2010.zip\n",
        "# !rm -rf //content/BDict2009.zip\n",
        "# !rm -rf //content/BDict2008.zip\n",
        "# !rm -rf //content/BDict2007.zip\n",
        "# !rm -rf //content/BDict2006.zip\n",
        "# !rm -rf //content/BDict2005.zip\n",
        "# !rm -rf //content/BDict2004.zip\n",
        "# !rm -rf //content/BDict2003.zip\n",
        "# !rm -rf //content/BDict2002.zip\n",
        "# !rm -rf //content/BDict2001.zip\n",
        "# !rm -rf //content/BDict2000.zip\n",
        "# !rm -rf //content/BDict1999.zip\n",
        "# !rm -rf //content/BDict1998.zip\n",
        "# !rm -rf //content/BDict1997.zip\n",
        "# !rm -rf //content/BDict1996.zip\n",
        "# !rm -rf //content/BDict1995.zip\n",
        "# !rm -rf //content/BDict1994.zip\n",
        "# !rm -rf //content/BDict1993.zip\n",
        "# !rm -rf //content/BDict1992.zip\n",
        "# !rm -rf //content/BDict1991.zip\n",
        "# !rm -rf //content/BDict1990.zip\n",
        "# !rm -rf //content/BDict1988.zip\n",
        "# !rm -rf //content/BDict1986.zip\n",
        "# !rm -rf //content/BDict1984.zip\n",
        "# !rm -rf //content/BDict1980.zip\n",
        "# !rm -rf //content/EDict2021.zip\n",
        "# !rm -rf //content/EDict2020.zip\n",
        "# !rm -rf //content/EDict2019.zip\n",
        "# !rm -rf //content/EDict2018.zip\n",
        "# !rm -rf //content/EDict2017.zip\n",
        "# !rm -rf //content/EDict2016.zip\n",
        "# !rm -rf //content/EDict2015.zip\n",
        "# !rm -rf //content/EDict2014.zip\n",
        "# !rm -rf //content/EDict2013.zip\n",
        "# !rm -rf //content/EDict2012.zip\n",
        "# !rm -rf //content/EDict2011.zip\n",
        "# !rm -rf //content/EDict2010.zip\n",
        "# !rm -rf //content/EDict2009.zip\n",
        "# !rm -rf //content/EDict2008.zip\n",
        "# !rm -rf //content/EDict2007.zip\n",
        "# !rm -rf //content/EDict2006.zip\n",
        "# !rm -rf //content/EDict2005.zip\n",
        "# !rm -rf //content/EDict2004.zip\n",
        "# !rm -rf //content/EDict2003.zip\n",
        "# !rm -rf //content/EDict2002.zip\n",
        "# !rm -rf //content/EDict2001.zip\n",
        "# !rm -rf //content/EDict2000.zip\n",
        "# !rm -rf //content/EDict1999.zip\n",
        "# !rm -rf //content/EDict1998.zip\n",
        "# !rm -rf //content/EDict1997.zip\n",
        "# !rm -rf //content/EDict1996.zip\n",
        "# !rm -rf //content/EDict1995.zip\n",
        "# !rm -rf //content/EDict1994.zip\n",
        "# !rm -rf //content/EDict1993.zip\n",
        "# !rm -rf //content/EDict1992.zip\n",
        "# !rm -rf //content/EDict1991.zip\n",
        "# !rm -rf //content/EDict1990.zip\n",
        "# !rm -rf //content/EDict1988.zip\n",
        "# !rm -rf //content/EDict1986.zip\n",
        "# !rm -rf //content/EDict1984.zip\n",
        "# !rm -rf //content/EDict1980.zip\n",
        "# !rm -rf //content/Basics2021.zip\n",
        "# !rm -rf //content/Basics2020.zip\n",
        "# !rm -rf //content/Basics2019.zip\n",
        "# !rm -rf //content/Basics2018.zip\n",
        "# !rm -rf //content/Basics2017.zip\n",
        "# !rm -rf //content/Basics2016.zip\n",
        "# !rm -rf //content/Basics2015.zip\n",
        "# !rm -rf //content/Basics2014.zip\n",
        "# !rm -rf //content/Basics2013.zip\n",
        "# !rm -rf //content/Basics2012.zip\n",
        "# !rm -rf //content/Basics2011.zip\n",
        "# !rm -rf //content/Basics2010.zip\n",
        "# !rm -rf //content/Basics2009.zip\n",
        "# !rm -rf //content/Basics2008.zip\n",
        "# !rm -rf //content/Basics2007.zip\n",
        "# !rm -rf //content/Basics2006.zip\n",
        "# !rm -rf //content/Basics2005.zip\n",
        "# !rm -rf //content/Basics2004.zip\n",
        "# !rm -rf //content/Basics2003.zip\n",
        "# !rm -rf //content/Basics2002.zip\n",
        "# !rm -rf //content/Basics2001.zip\n",
        "# !rm -rf //content/Basics2000.zip\n",
        "# !rm -rf //content/Basics1999.zip\n",
        "# !rm -rf //content/Basics1998.zip\n",
        "# !rm -rf //content/Basics1997.zip\n",
        "# !rm -rf //content/Basics1996.zip\n",
        "# !rm -rf //content/Basics1995.zip\n",
        "# !rm -rf //content/Basics1994.zip\n",
        "# !rm -rf //content/Basics1993.zip\n",
        "# !rm -rf //content/Basics1992.zip\n",
        "# !rm -rf //content/Basics1991.zip\n",
        "# !rm -rf //content/Basics1990.zip\n",
        "# !rm -rf //content/Basics1988.zip\n",
        "# !rm -rf //content/Basics1986.zip\n",
        "# !rm -rf //content/Basics1984.zip\n",
        "# !rm -rf //content/Basics1980.zip\n",
        "# !rm -rf //content/Ethnicity2021.zip\n",
        "# !rm -rf //content/Ethnicity2020.zip\n",
        "# !rm -rf //content/Ethnicity2019.zip\n",
        "# !rm -rf //content/Ethnicity2018.zip\n",
        "# !rm -rf //content/Ethnicity2017.zip\n",
        "# !rm -rf //content/Ethnicity2016.zip\n",
        "# !rm -rf //content/Ethnicity2015.zip\n",
        "# !rm -rf //content/Ethnicity2014.zip\n",
        "# !rm -rf //content/Ethnicity2013.zip\n",
        "# !rm -rf //content/Ethnicity2012.zip\n",
        "# !rm -rf //content/Ethnicity2011.zip\n",
        "# !rm -rf //content/Ethnicity2010.zip\n",
        "# !rm -rf //content/Ethnicity2009.zip\n",
        "# !rm -rf //content/Ethnicity2008.zip\n",
        "# !rm -rf //content/Ethnicity2007.zip\n",
        "# !rm -rf //content/Ethnicity2006.zip\n",
        "# !rm -rf //content/Ethnicity2005.zip\n",
        "# !rm -rf //content/Ethnicity2004.zip\n",
        "# !rm -rf //content/Ethnicity2003.zip\n",
        "# !rm -rf //content/Ethnicity2002.zip\n",
        "# !rm -rf //content/Ethnicity2001.zip\n",
        "# !rm -rf //content/Ethnicity2000.zip\n",
        "# !rm -rf //content/Ethnicity1999.zip\n",
        "# !rm -rf //content/Ethnicity1998.zip\n",
        "# !rm -rf //content/Ethnicity1997.zip\n",
        "# !rm -rf //content/Ethnicity1996.zip\n",
        "# !rm -rf //content/Ethnicity1995.zip\n",
        "# !rm -rf //content/Ethnicity1994.zip\n",
        "# !rm -rf //content/Ethnicity1993.zip\n",
        "# !rm -rf //content/Ethnicity1992.zip\n",
        "# !rm -rf //content/Ethnicity1991.zip\n",
        "# !rm -rf //content/Ethnicity1990.zip\n",
        "# !rm -rf //content/Ethnicity1988.zip\n",
        "# !rm -rf //content/Ethnicity1986.zip\n",
        "# !rm -rf //content/Ethnicity1984.zip\n",
        "# !rm -rf //content/Ethnicity1980.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "KJZe4sitfMhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create names for all needed urls\n",
        "\n",
        "Basics2021_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2021.zip'\n",
        "Ethnicity2021_url = 'https://nces.ed.gov/ipeds/datacenter/data/EFFY2021.zip'\n",
        "Basics2020_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2020.zip'\n",
        "Ethnicity2020_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2020A.zip'\n",
        "Basics2019_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2019.zip'\n",
        "Ethnicity2019_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2019A.zip'\n",
        "Basics2018_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2018.zip'\n",
        "Ethnicity2018_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2018A.zip'\n",
        "Basics2017_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2017.zip'\n",
        "Ethnicity2017_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2017A.zip'\n",
        "Basics2016_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2016.zip'\n",
        "Ethnicity2016_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2016A.zip'\n",
        "Basics2015_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2015.zip'\n",
        "Ethnicity2015_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2015A.zip'\n",
        "Basics2014_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2014.zip'\n",
        "Ethnicity2014_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2014A.zip'\n",
        "Basics2013_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2013.zip'\n",
        "Ethnicity2013_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2013A.zip'\n",
        "Basics2012_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2012.zip'\n",
        "Ethnicity2012_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2012A.zip'\n",
        "Basics2011_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2011.zip'\n",
        "Ethnicity2011_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2011A.zip'\n",
        "Basics2010_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2010.zip'\n",
        "Ethnicity2010_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2010A.zip'\n",
        "Basics2009_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2009.zip'\n",
        "Ethnicity2009_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2009A.zip'\n",
        "Basics2008_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2008.zip'\n",
        "Ethnicity2008_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2008A.zip'\n",
        "Basics2007_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2007.zip'\n",
        "Ethnicity2007_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2007A.zip'\n",
        "Basics2006_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2006.zip'\n",
        "Ethnicity2006_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2006A.zip'\n",
        "Basics2005_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2005.zip'\n",
        "Ethnicity2005_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2005A.zip'\n",
        "Basics2004_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2004.zip'\n",
        "Ethnicity2004_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2004A.zip'\n",
        "Basics2003_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2003.zip'\n",
        "Ethnicity2003_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2003A.zip'\n",
        "Basics2002_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2002.zip'\n",
        "Ethnicity2002_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2002A.zip'\n",
        "Basics2001_url = 'https://nces.ed.gov/ipeds/datacenter/data/FA2001HD.zip'\n",
        "Ethnicity2001_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2001A.zip'\n",
        "Basics2000_url = 'https://nces.ed.gov/ipeds/datacenter/data/FA2000HD.zip'\n",
        "Ethnicity2000_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2000A.zip'\n",
        "Basics1999_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC99_HD.zip'\n",
        "Ethnicity1999_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF99_ANR.zip'\n",
        "Basics1998_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC98hdac.zip'\n",
        "Ethnicity1998_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF98_ANR.zip'\n",
        "Basics1997_url = 'https://nces.ed.gov/ipeds/datacenter/data/ic9798_HDR.zip'\n",
        "Ethnicity1997_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF97_ANR.zip'\n",
        "Basics1996_url = 'https://nces.ed.gov/ipeds/datacenter/data/ic9697_A.zip'\n",
        "Ethnicity1996_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF96_ANR.zip'\n",
        "Basics1995_url = 'https://nces.ed.gov/ipeds/datacenter/data/ic9596_A.zip'\n",
        "Ethnicity1995_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF95_ANR.zip'\n",
        "Basics1994_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1994_A.zip'\n",
        "Ethnicity1994_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1994_ANR.zip'\n",
        "Basics1993_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1993_A.zip'\n",
        "Ethnicity1993_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1993_A.zip'\n",
        "Basics1992_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1992_A.zip'\n",
        "Ethnicity1992_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1992_A.zip'\n",
        "Basics1991_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1991_hdr.zip'\n",
        "Ethnicity1991_url = 'https://nces.ed.gov/ipeds/datacenter/data/ef1991_a.zip'\n",
        "Basics1990_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC90HD.zip'\n",
        "Ethnicity1990_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF90_A.zip'\n",
        "Basics1988_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1988_A.zip'\n",
        "Ethnicity1988_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1988_A.zip'\n",
        "Basics1986_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1986_A.zip'\n",
        "Ethnicity1986_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1986_A.zip'\n",
        "Basics1984_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1984.zip'\n",
        "Ethnicity1984_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1984.zip'\n",
        "Basics1980_url = 'https://nces.ed.gov/ipeds/datacenter/data/IC1980.zip'\n",
        "Ethnicity1980_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF1980_A.zip'\n",
        "\n",
        "BDict2021_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2021_Dict.zip'\n",
        "EDict2021_url = 'https://nces.ed.gov/ipeds/datacenter/data/EFFY2021_Dict.zip'\n",
        "BDict2020_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2020_Dict.zip'\n",
        "EDict2020_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2020A_Dict.zip'\n",
        "BDict2019_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2019_Dict.zip'\n",
        "EDict2019_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2019A_Dict.zip'\n",
        "BDict2018_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2018_Dict.zip'\n",
        "EDict2018_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2018A_Dict.zip'\n",
        "BDict2017_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2017_Dict.zip'\n",
        "EDict2017_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2017A_Dict.zip'\n",
        "BDict2016_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2016_Dict.zip'\n",
        "EDict2016_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2016A_Dict.zip'\n",
        "BDict2015_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2015_Dict.zip'\n",
        "EDict2015_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2015A_Dict.zip'\n",
        "BDict2014_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2014_Dict.zip'\n",
        "EDict2014_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2014A_Dict.zip'\n",
        "BDict2013_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2013_Dict.zip'\n",
        "EDict2013_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2013A_Dict.zip'\n",
        "BDict2012_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2012_Dict.zip'\n",
        "EDict2012_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2012A_Dict.zip'\n",
        "BDict2011_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2011_Dict.zip'\n",
        "EDict2011_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2011A_Dict.zip'\n",
        "BDict2010_url = 'https://nces.ed.gov/ipeds/datacenter/data/HD2010_Dict.zip'\n",
        "EDict2010_url = 'https://nces.ed.gov/ipeds/datacenter/data/EF2010A_Dict.zip'"
      ],
      "metadata": {
        "id": "veE_y2JFfMkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists of url names\n",
        "\n",
        "myurlslist = [Ethnicity1980_url, Basics1980_url, Ethnicity1984_url, Basics1984_url, Ethnicity1986_url, Basics1986_url, Ethnicity1988_url, Basics1988_url, Ethnicity1990_url, Basics1990_url, Ethnicity1991_url, Basics1991_url, Ethnicity1992_url, Basics1992_url, Ethnicity1993_url, Basics1993_url, Ethnicity1994_url, Basics1994_url, Ethnicity1995_url, Basics1995_url, Ethnicity1996_url, Basics1996_url, Ethnicity1997_url, Basics1997_url, Ethnicity1998_url, Basics1998_url, Ethnicity1999_url, Basics1999_url, Ethnicity2000_url, Basics2000_url, Ethnicity2001_url, Basics2001_url, Ethnicity2002_url, Basics2002_url, Ethnicity2003_url, Basics2003_url, Ethnicity2004_url, Basics2004_url, Ethnicity2005_url, Basics2005_url, Ethnicity2006_url, Basics2006_url, Ethnicity2007_url, Basics2007_url, Ethnicity2008_url, Basics2008_url, Ethnicity2009_url, Basics2009_url, Ethnicity2010_url, Basics2010_url, Ethnicity2011_url, Basics2011_url, Ethnicity2012_url, Basics2012_url, Ethnicity2013_url, Basics2013_url, Ethnicity2014_url, Basics2014_url, Ethnicity2015_url, Basics2015_url, Ethnicity2016_url, Basics2016_url, Ethnicity2017_url, Basics2017_url, Ethnicity2018_url, Basics2018_url, Ethnicity2019_url, Basics2019_url, Ethnicity2020_url, Basics2020_url, Ethnicity2021_url, Basics2021_url]\n",
        "myurlslistnames = ['Ethnicity1980', 'Basics1980', 'Ethnicity1984', 'Basics1984', 'Ethnicity1986', 'Basics1986', 'Ethnicity1988', 'Basics1988', 'Ethnicity1990', 'Basics1990', 'Ethnicity1991', 'Basics1991', 'Ethnicity1992', 'Basics1992', 'Ethnicity1993', 'Basics1993', 'Ethnicity1994', 'Basics1994', 'Ethnicity1995', 'Basics1995', 'Ethnicity1996', 'Basics1996', 'Ethnicity1997', 'Basics1997', 'Ethnicity1998', 'Basics1998', 'Ethnicity1999', 'Basics1999', 'Ethnicity2000', 'Basics2000', 'Ethnicity2001', 'Basics2001', 'Ethnicity2002', 'Basics2002', 'Ethnicity2003', 'Basics2003', 'Ethnicity2004', 'Basics2004', 'Ethnicity2005', 'Basics2005', 'Ethnicity2006', 'Basics2006', 'Ethnicity2007', 'Basics2007', 'Ethnicity2008', 'Basics2008', 'Ethnicity2009', 'Basics2009', 'Ethnicity2010', 'Basics2010', 'Ethnicity2011', 'Basics2011', 'Ethnicity2012', 'Basics2012', 'Ethnicity2013', 'Basics2013', 'Ethnicity2014', 'Basics2014', 'Ethnicity2015', 'Basics2015', 'Ethnicity2016', 'Basics2016', 'Ethnicity2017', 'Basics2017', 'Ethnicity2018', 'Basics2018', 'Ethnicity2019', 'Basics2019', 'Ethnicity2020', 'Basics2020', 'Ethnicity2021', 'Basics2021']\n",
        "\n",
        "myDicturlslist = [EDict2010_url, BDict2010_url, EDict2011_url, BDict2011_url, EDict2012_url, BDict2012_url, EDict2013_url, BDict2013_url, EDict2014_url, BDict2014_url, EDict2015_url, BDict2015_url, EDict2016_url, BDict2016_url, EDict2017_url, BDict2017_url, EDict2018_url, BDict2018_url, EDict2019_url, BDict2019_url, EDict2020_url, BDict2020_url, EDict2021_url, BDict2021_url]\n",
        "myDicturlslistnames = ['EDict2010', 'BDict2010', 'EDict2011', 'BDict2011', 'EDict2012', 'BDict2012', 'EDict2013', 'BDict2013', 'EDict2014', 'BDict2014', 'EDict2015', 'BDict2015', 'EDict2016', 'BDict2016', 'EDict2017', 'BDict2017', 'EDict2018', 'BDict2018', 'EDict2019', 'BDict2019', 'EDict2020', 'BDict2020', 'EDict2021', 'BDict2021']\n"
      ],
      "metadata": {
        "id": "8g_1PZynfMnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes of url names lists \n",
        "\n",
        "df = pd.DataFrame(myurlslist,index=myurlslistnames)\n",
        "dfDict = pd.DataFrame(myDicturlslist,index=myDicturlslistnames)"
      ],
      "metadata": {
        "id": "amwW8SAufMqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get zip files from urls \n",
        "# NOTE: This cell takes about five to eight minutes due to size of files\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "response = requests.get(Basics2021_url, stream=True)\n",
        "with open('Basics2021.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2021_url, stream=True)\n",
        "with open('Ethnicity2021.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2020_url, stream=True)\n",
        "with open('Basics2020.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2020_url, stream=True)\n",
        "with open('Ethnicity2020.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2019_url, stream=True)\n",
        "with open('Basics2019.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2019_url, stream=True)\n",
        "with open('Ethnicity2019.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2018_url, stream=True)\n",
        "with open('Basics2018.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2018_url, stream=True)\n",
        "with open('Ethnicity2018.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2017_url, stream=True)\n",
        "with open('Basics2017.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2017_url, stream=True)\n",
        "with open('Ethnicity2017.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2016_url, stream=True)\n",
        "with open('Basics2016.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2016_url, stream=True)\n",
        "with open('Ethnicity2016.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2015_url, stream=True)\n",
        "with open('Basics2015.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2015_url, stream=True)\n",
        "with open('Ethnicity2015.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2014_url, stream=True)\n",
        "with open('Basics2014.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2014_url, stream=True)\n",
        "with open('Ethnicity2014.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2013_url, stream=True)\n",
        "with open('Basics2013.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2013_url, stream=True)\n",
        "with open('Ethnicity2013.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2012_url, stream=True)\n",
        "with open('Basics2012.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2012_url, stream=True)\n",
        "with open('Ethnicity2012.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2011_url, stream=True)\n",
        "with open('Basics2011.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2011_url, stream=True)\n",
        "with open('Ethnicity2011.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2010_url, stream=True)\n",
        "with open('Basics2010.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2010_url, stream=True)\n",
        "with open('Ethnicity2010.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2009_url, stream=True)\n",
        "with open('Basics2009.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2009_url, stream=True)\n",
        "with open('Ethnicity2009.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2008_url, stream=True)\n",
        "with open('Basics2008.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2008_url, stream=True)\n",
        "with open('Ethnicity2008.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2007_url, stream=True)\n",
        "with open('Basics2007.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2007_url, stream=True)\n",
        "with open('Ethnicity2007.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2006_url, stream=True)\n",
        "with open('Basics2006.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2006_url, stream=True)\n",
        "with open('Ethnicity2006.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2005_url, stream=True)\n",
        "with open('Basics2005.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2005_url, stream=True)\n",
        "with open('Ethnicity2005.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2004_url, stream=True)\n",
        "with open('Basics2004.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2004_url, stream=True)\n",
        "with open('Ethnicity2004.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2003_url, stream=True)\n",
        "with open('Basics2003.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2003_url, stream=True)\n",
        "with open('Ethnicity2003.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2002_url, stream=True)\n",
        "with open('Basics2002.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2002_url, stream=True)\n",
        "with open('Ethnicity2002.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2001_url, stream=True)\n",
        "with open('Basics2001.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2001_url, stream=True)\n",
        "with open('Ethnicity2001.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics2000_url, stream=True)\n",
        "with open('Basics2000.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity2000_url, stream=True)\n",
        "with open('Ethnicity2000.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1999_url, stream=True)\n",
        "with open('Basics1999.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1999_url, stream=True)\n",
        "with open('Ethnicity1999.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1998_url, stream=True)\n",
        "with open('Basics1998.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1998_url, stream=True)\n",
        "with open('Ethnicity1998.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1997_url, stream=True)\n",
        "with open('Basics1997.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1997_url, stream=True)\n",
        "with open('Ethnicity1997.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1996_url, stream=True)\n",
        "with open('Basics1996.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1996_url, stream=True)\n",
        "with open('Ethnicity1996.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1995_url, stream=True)\n",
        "with open('Basics1995.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1995_url, stream=True)\n",
        "with open('Ethnicity1995.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1994_url, stream=True)\n",
        "with open('Basics1994.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1994_url, stream=True)\n",
        "with open('Ethnicity1994.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1993_url, stream=True)\n",
        "with open('Basics1993.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1993_url, stream=True)\n",
        "with open('Ethnicity1993.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1992_url, stream=True)\n",
        "with open('Basics1992.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1992_url, stream=True)\n",
        "with open('Ethnicity1992.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1991_url, stream=True)\n",
        "with open('Basics1991.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1991_url, stream=True)\n",
        "with open('Ethnicity1991.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1990_url, stream=True)\n",
        "with open('Basics1990.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1990_url, stream=True)\n",
        "with open('Ethnicity1990.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1988_url, stream=True)\n",
        "with open('Basics1988.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1988_url, stream=True)\n",
        "with open('Ethnicity1988.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1986_url, stream=True)\n",
        "with open('Basics1986.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1986_url, stream=True)\n",
        "with open('Ethnicity1986.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1984_url, stream=True)\n",
        "with open('Basics1984.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1984_url, stream=True)\n",
        "with open('Ethnicity1984.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Basics1980_url, stream=True)\n",
        "with open('Basics1980.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(Ethnicity1980_url, stream=True)\n",
        "with open('Ethnicity1980.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "rRdEl3tefMuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get more zip files from urls \n",
        "# NOTE: This cell doesn't take as long because files are smaller\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "response = requests.get(BDict2021_url, stream=True)\n",
        "with open('BDict2021.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2021_url, stream=True)\n",
        "with open('EDict2021.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2020_url, stream=True)\n",
        "with open('BDict2020.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2020_url, stream=True)\n",
        "with open('EDict2020.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2019_url, stream=True)\n",
        "with open('BDict2019.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2019_url, stream=True)\n",
        "with open('EDict2019.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2018_url, stream=True)\n",
        "with open('BDict2018.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2018_url, stream=True)\n",
        "with open('EDict2018.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2017_url, stream=True)\n",
        "with open('BDict2017.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2017_url, stream=True)\n",
        "with open('EDict2017.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2016_url, stream=True)\n",
        "with open('BDict2016.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2016_url, stream=True)\n",
        "with open('EDict2016.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2015_url, stream=True)\n",
        "with open('BDict2015.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2015_url, stream=True)\n",
        "with open('EDict2015.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2014_url, stream=True)\n",
        "with open('BDict2014.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2014_url, stream=True)\n",
        "with open('EDict2014.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2013_url, stream=True)\n",
        "with open('BDict2013.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2013_url, stream=True)\n",
        "with open('EDict2013.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2012_url, stream=True)\n",
        "with open('BDict2012.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2012_url, stream=True)\n",
        "with open('EDict2012.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2011_url, stream=True)\n",
        "with open('BDict2011.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2011_url, stream=True)\n",
        "with open('EDict2011.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(BDict2010_url, stream=True)\n",
        "with open('BDict2010.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "response = requests.get(EDict2010_url, stream=True)\n",
        "with open('EDict2010.zip', \"wb\") as f:\n",
        "      for chunk in response.iter_content(chunk_size=2048):\n",
        "          if chunk:  # filter out keep-alive new chunks\n",
        "              f.write(chunk)\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")\n"
      ],
      "metadata": {
        "id": "LBC9Fv5NfMxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists of zip files \n",
        "\n",
        "folders = ['Ethnicity1980.zip', 'Basics1980.zip', 'Ethnicity1984.zip', 'Basics1984.zip', 'Ethnicity1986.zip', 'Basics1986.zip', 'Ethnicity1988.zip', 'Basics1988.zip', 'Ethnicity1990.zip', 'Basics1990.zip', 'Ethnicity1991.zip', 'Basics1991.zip', 'Ethnicity1992.zip', 'Basics1992.zip', 'Ethnicity1993.zip', 'Basics1993.zip', 'Ethnicity1994.zip', 'Basics1994.zip', 'Ethnicity1995.zip', 'Basics1995.zip', 'Ethnicity1996.zip', 'Basics1996.zip', 'Ethnicity1997.zip', 'Basics1997.zip', 'Ethnicity1998.zip', 'Basics1998.zip', 'Ethnicity1999.zip', 'Basics1999.zip', 'Ethnicity2000.zip', 'Basics2000.zip', 'Ethnicity2001.zip', 'Basics2001.zip', 'Ethnicity2002.zip', 'Basics2002.zip', 'Ethnicity2003.zip', 'Basics2003.zip', 'Ethnicity2004.zip', 'Basics2004.zip', 'Ethnicity2005.zip', 'Basics2005.zip', 'Ethnicity2006.zip', 'Basics2006.zip', 'Ethnicity2007.zip', 'Basics2007.zip', 'Ethnicity2008.zip', 'Basics2008.zip', 'Ethnicity2009.zip', 'Basics2009.zip', 'Ethnicity2010.zip', 'Basics2010.zip', 'Ethnicity2011.zip', 'Basics2011.zip', 'Ethnicity2012.zip', 'Basics2012.zip', 'Ethnicity2013.zip', 'Basics2013.zip', 'Ethnicity2014.zip', 'Basics2014.zip', 'Ethnicity2015.zip', 'Basics2015.zip', 'Ethnicity2016.zip', 'Basics2016.zip', 'Ethnicity2017.zip', 'Basics2017.zip', 'Ethnicity2018.zip', 'Basics2018.zip', 'Ethnicity2019.zip', 'Basics2019.zip', 'Ethnicity2020.zip', 'Basics2020.zip', 'Ethnicity2021.zip', 'Basics2021.zip']\n",
        "foldersDict = ['EDict2010.zip', 'BDict2010.zip', 'EDict2011.zip', 'BDict2011.zip', 'EDict2012.zip', 'BDict2012.zip', 'EDict2013.zip', 'BDict2013.zip', 'EDict2014.zip', 'BDict2014.zip', 'EDict2015.zip', 'BDict2015.zip', 'EDict2016.zip', 'BDict2016.zip', 'EDict2017.zip', 'BDict2017.zip', 'EDict2018.zip', 'BDict2018.zip', 'EDict2019.zip', 'BDict2019.zip', 'EDict2020.zip', 'BDict2020.zip', 'EDict2021.zip', 'BDict2021.zip']\n"
      ],
      "metadata": {
        "id": "KfpzCdUDfM09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cvs files (basic and ethnicity data) from zip files, rename them after title of zip file and put them into IPEDS_Folder\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for folder in folders:\n",
        "  folder_name = folder\n",
        "  with ZipFile(folder_name, 'r') as zipObj:\n",
        "    file_names = zipObj.namelist()\n",
        "    for file_name in file_names:\n",
        "      if file_name.endswith('.csv'):\n",
        "        zipObj.extract(file_name, 'IPEDS_Folder')\n",
        "        os.rename('/content/IPEDS_Folder/' + file_name, '/content/IPEDS_Folder/' + folder_name[:len(folder_name) - 4] + '.csv')\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")"
      ],
      "metadata": {
        "id": "Hz1hqpQYfM4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get needed main dictionary sheets from xls files and xlsx files in zip files, rename them after title of zip file and put them into IPEDS_FolderDict\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "os.chdir('/content/')  # If this cells throws an error, enable or disable this line of code and try again\n",
        "\n",
        "for folder in foldersDict:\n",
        "  folder_name = folder\n",
        "  with ZipFile(folder_name, 'r') as zipObj:\n",
        "    file_names = zipObj.namelist()\n",
        "    for file_name in file_names:\n",
        "      if file_name.endswith('.xls'):\n",
        "        zipObj.extract(file_name, 'IPEDS_FolderDict')\n",
        "        os.rename('/content/IPEDS_FolderDict/' + file_name, '/content/IPEDS_FolderDict/' + folder_name[:len(folder_name) - 4] + '.xls')\n",
        "\n",
        "for folder in foldersDict:\n",
        "  folder_name = folder\n",
        "  with ZipFile(folder_name, 'r') as zipObj:\n",
        "    file_names = zipObj.namelist()\n",
        "    for file_name in file_names:\n",
        "      if file_name.endswith('.xlsx'):\n",
        "        zipObj.extract(file_name, 'IPEDS_FolderDict')\n",
        "        os.rename('/content/IPEDS_FolderDict/' + file_name, '/content/IPEDS_FolderDict/' + folder_name[:len(folder_name) - 4] + '.xls')\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")"
      ],
      "metadata": {
        "id": "hp5xM-VnfM74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## glob files together into lists, breaking them up by decade to keep the files to be created below from getting too big.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "os.chdir('/content/IPEDS_Folder') # If this cells throws an error, enable this line of code and try again\n",
        "myfilesEth2020s = glob.glob('Ethnicity202*.csv')\n",
        "myfilesEth2010s = glob.glob('Ethnicity201*.csv')\n",
        "myfilesEth1990s = glob.glob('Ethnicity199*.csv')\n",
        "myfilesEth1980s = glob.glob('Ethnicity198*.csv')\n",
        "myfilesBas2020s = glob.glob('Basics202*.csv')\n",
        "myfilesBas2010s = glob.glob('Basics201*.csv')\n",
        "myfilesBas1990s = glob.glob('Basics199*.csv')\n",
        "myfilesBas1980s = glob.glob('Basics198*.csv')\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")"
      ],
      "metadata": {
        "id": "9r74NlUvfM_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## glob all dictionary files together into a list /content/IPEDS_FolderDict\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "os.chdir('/content/IPEDS_FolderDict')  # If this cells throws an error, enable or disable this line of code and try again\n",
        "myfilesDictBasics = glob.glob('B*.xl*')\n",
        "myfilesDictEthnics = glob.glob('E*.xl*')\n",
        "\n",
        "end = time.time()\n",
        "print(\"that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")"
      ],
      "metadata": {
        "id": "NtP2djlBfNCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create single spreadsheet of dictionary sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "# os.chdir('/content/IPEDS_FolderDict')  # If this cells throws an error, enable or disable this line of code and try again\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSDict.xlsx\") as writer:\n",
        "  for file in myfilesDictEthnics:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_excel(file, sheet_name = 1)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "  for file in myfilesDictBasics:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_excel(file, sheet_name = 1)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "end = time.time()\n",
        "\n",
        "\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/1)\n",
        "print(f\"seconds\")"
      ],
      "metadata": {
        "id": "9MUGUtwPfNF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes three to five minutes due to size of files.\n",
        "# Create single spreadsheet of 2020s ethnicity sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "os.chdir('/content/IPEDS_Folder')  # If this cells throws an error, enable or disable this line of code and try again\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSEth2020s.xlsx\") as writer:\n",
        "  for file in myfilesEth2020s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "v_oMGyZ8fNJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes 20+ minutes due to size of files.\n",
        "# Create single spreadsheet of 2010s ethnicity sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSEth2010s.xlsx\") as writer:\n",
        "  for file in myfilesEth2010s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "FjbOdSP2fNMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes six or seven minutes due to size of files.\n",
        "# Create single spreadsheet of 1990s ethnicity sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSEth1990s.xlsx\") as writer:\n",
        "  for file in myfilesEth1990s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "x3cfRr3JfNQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes less than two minutes.\n",
        "# Create single spreadsheet of 1980s ethnicity sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSEth1980s.xlsx\") as writer:\n",
        "  for file in myfilesEth1980s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "pM603aJ_fNTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes less than a minute.\n",
        "# Create single spreadsheet of 2020s basic sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSBas2020s.xlsx\",engine='xlsxwriter') as writer:\n",
        "  for file in myfilesBas2020s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file, encoding_errors='ignore', low_memory=False)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "t9fOKe_pguHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes up to two minutes due to size of files.\n",
        "# Create single spreadsheet of 2010s basic sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSBas2010s.xlsx\", engine='xlsxwriter') as writer:\n",
        "  for file in myfilesBas2010s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file, encoding_errors='ignore', low_memory=False)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "XVYZxkZxguKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes up to three minutes due to size of files.\n",
        "# Create single spreadsheet of 1990s basic sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "# NOTE: NEEDS THIS IN read_csv LINE: encoding_errors='ignore', low_memory=False\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSBas1990s.xlsx\") as writer:\n",
        "  for file in myfilesBas1990s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file, encoding_errors='ignore', low_memory=False)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "l-NaQk4UguNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell takes a minute.\n",
        "# Create single spreadsheet of 1980s basic sheets\n",
        "# run through pandas excelwriter (not pure python's)\n",
        "# the actual file name gets used as the sheet name so we can track it.\n",
        "# NOTE: NEEDS THIS IN read_csv LINE: encoding_errors='ignore', low_memory=False\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "with pd.ExcelWriter(\"IPEDSBas1980s.xlsx\") as writer: #, engine='xlsxwriter'\n",
        "  for file in myfilesBas1980s:\n",
        "    file_name = f\"df_{file}\"\n",
        "    file_name = pd.read_csv(file, encoding_errors='ignore', low_memory=False)\n",
        "    print(f'Writing {file} as a sheet to xlsx file')\n",
        "    file_name.to_excel(writer, sheet_name = file[:len(file) - 4], index = False)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"done combining individual sheets in a xlsx workbook. that took...\")\n",
        "print(math.ceil(end - start)/60)\n",
        "print(f\"minutes\")"
      ],
      "metadata": {
        "id": "wKqIYrUgguQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files \n",
        "# NOTE: NEED TO CLICK BOX THAT SAYS ALLOW COLAB TO DOWNLOAD MULTIPLE FILES. \n",
        "\n",
        "files.download('IPEDSBas1980s.xlsx') \n",
        "files.download('IPEDSBas1990s.xlsx') \n",
        "files.download('IPEDSBas2010s.xlsx') \n",
        "files.download('IPEDSBas2020s.xlsx') \n",
        "\n"
      ],
      "metadata": {
        "id": "euYvB19Sguo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download more files\n",
        "\n",
        "files.download('IPEDSEth1980s.xlsx') \n",
        "files.download('IPEDSEth1990s.xlsx') \n",
        "files.download('IPEDSEth2010s.xlsx') \n",
        "files.download('IPEDSEth2020s.xlsx') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "PDHAAYaqgusd",
        "outputId": "15157680-2872-44ec-cfb2-b313f1ffce32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7c4e90df-c183-46d3-883b-a26d4449c16a\", \"IPEDSEth1980s.xlsx\", 16658351)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9372acb5-8e25-47db-bc81-fd8c158fb85a\", \"IPEDSEth1990s.xlsx\", 86948199)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_de96f17c-a3f1-4bbf-9fed-90ccff30b846\", \"IPEDSEth2010s.xlsx\", 280460221)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4be5d2b6-c868-4e9b-9197-b025e839bbea\", \"IPEDSEth2020s.xlsx\", 46557077)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download last file\n",
        "\n",
        "os.chdir('/content/IPEDS_FolderDict')  # If this cells throws an error, enable or disable this line of code and try again\n",
        "files.download('IPEDSDict.xlsx') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7rXqb7OBguvh",
        "outputId": "579e677b-75c6-4c3b-f6c3-4093a4e4a001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_12e77926-1362-4457-835d-1a6e8bd78e58\", \"IPEDSDict.xlsx\", 57490)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SxfgryeqfNWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}